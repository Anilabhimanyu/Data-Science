{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBFokYZI5mwWZaFJWR3BwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilabhimanyu/Data-Science/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCWGinqtSmxO"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "1. Data Preprocessing is the process of converting raw data into the most useful information, and understandable format\n",
        "2. Important step in data science as we can't work with raw data\n",
        "3. quality of the data should be checked before making the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is Data Preprocessing\n",
        "1. Accuaracy: whether data entered is correct or not\n",
        "2. Completeness: whether data is available or not recorded\n",
        "3. Timeliness: data should be updated correctly\n",
        "4. consistency: whether same data is kept in all the places that do or do not match\n",
        "5. Interpretability: should be understandable\n",
        "6. Believability: should be trustable & believable"
      ],
      "metadata": {
        "id": "3C-anu0ITnb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tasks under data preprocessing\n",
        "1. Understanding the data requirements\n",
        "2. Exploring the data and finding errors & faults & issues\n",
        "3. Addressing the solutions for issues\n",
        "\n",
        "# Major Tasks in Data Preprocessing\n",
        "1. Data Cleaning\n",
        "2. Data integration\n",
        "3. Data Reduction\n",
        "4. Data Transformation"
      ],
      "metadata": {
        "id": "F1-j5Jz-U1B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "yXPaSX1CVTXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a. Irrelevant Data\n",
        "solution: \n",
        "1. removing column\n",
        "2. Make the column useful for the model\n",
        "Requirements:\n",
        "1. subject matter expert to determine whether data is useful or not\n",
        "------------------------------------------------------------------\n",
        "b. Duplicate Data\n",
        "solution:\n",
        "1. duplicate rows and columns would hamper the analysis process\n",
        "2. removing the duplicate data\n",
        "------------------------------------------------------------------\n",
        "c. Noisy Data\n",
        "solution:\n",
        "1. depends on the type of noise involved\n",
        "2. large dataset requires automation filtering mechanism.\n",
        "    Ex: persons age 200\n",
        "3. other case deals with categorical values due to inconsistent entries.\n",
        "    Ex: country name as bharath, india and IND etc \n",
        "4. should be standardize the categories.\n",
        "Note: if error is not corrected, istead of removing the data, simply we make them as missing value\n",
        "------------------------------------------------------------------\n",
        "\n",
        "d. Incorrect DataType\n",
        "solution:\n",
        "1. Data stored in the incorrect datatype can be handled before analysis,\n",
        "------------------------------------------------------------------\n",
        "\n",
        "e. Missing Value\n",
        "1. ML models can't accept the missing values in a dataset.,\n",
        "2. Three aproaches:\n",
        "   a. dropping values\n",
        "   b. interpolation by mean, median for numerical and mode for categorical\n",
        "   c. with defauld value or adding indicator variable to flag.\n",
        "------------------------------------------------------------------\n",
        "\n",
        "f. MultiCollinearity\n",
        "solution:\n",
        "1. highly correlated data should be removed otherwise for linear models it raise errors.\n",
        "2. keeping the most relevant features in the model to avoid correlated predictor variables\n",
        "------------------------------------------------------------------\n",
        "\n",
        "g. Outliers\n",
        "solution:\n",
        "1. retained or removed based on models, like trees & boosting they are not sensitive to outliers.\n",
        "2. Winsorizing, i.e, the replacement of outliers with above or below a threshold by the nearest threshold values.\n",
        "------------------------------------------------------------------\n",
        "\n",
        "h. Unacceptable Format\n",
        "solution:\n",
        "1. It involves creating dummy variables, scaling, normailization & dimensionality redcution\n",
        "2. scaling means getting data into the same scale\n",
        "------------------------------------------------------------------\n",
        "\n",
        "i. Too many dimensions\n",
        "problems:\n",
        "1. curse of dimensionality(Hughes Phenomenon which says for the fixed sized dataset if we increases the dimensions then the performance will decreases)\n",
        "2. \n",
        "solution: \n",
        "1. garbage in garbage out, but Data science says as data increases the accuracy will increases but what if the dimensions of data is more.\n",
        "2. Removing unnecessary throught the inspection\n",
        "3. removing based on multicollinearity.\n",
        "4. using principle component analysis.\n",
        "------------------------------------------------------------------\n",
        "\n",
        "j. Too many Categories\n",
        "\n",
        "solution:\n",
        "1. what if the target contains too many categories, In a skewed distribution the presence of more frequented data can show significant impact\n",
        "on the less presented data.\n",
        "2. It is better to consider the all less appered as others.\n"
      ],
      "metadata": {
        "id": "nzFEZQ7GWH6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "mVlO26_nvTh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}