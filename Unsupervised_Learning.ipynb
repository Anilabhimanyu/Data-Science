{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk5XrOQm5MYUR1q5t4t5WS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilabhimanyu/Data-Science/blob/main/Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsupervised learning\n",
        "1. clustering\n",
        "2. associaton\n",
        "3. dimesionality reduction"
      ],
      "metadata": {
        "id": "tIeynCZ8i2_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering\n",
        "#--> grouping unlabled data into groups based on their similarity and differences\n",
        "#--> process raw data and unclassified data objects into groups represented by structures or patters in the information\n",
        "\n",
        "types:\n",
        "1. exclusive\n",
        "2. overlapping\n",
        "3. hierarchical\n",
        "4. probabilistic"
      ],
      "metadata": {
        "id": "VohxEzNti6lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Exclusive and overlapping clustering\n",
        "#--> exclusive means data point exists only in one cluster\n",
        "#--> also termed as 'hard clustering'\n",
        "\n",
        "\n",
        "ex: K Means Clustering\n",
        "#k means no of clusters\n",
        "#larger k means smaller groupins while larger granularity\n",
        "#smaller k means larger groupings while low granularity\n",
        "mostly used in \n",
        "1.market segmentation \n",
        "2.document clustering\n",
        "3.image segmentation\n",
        "4.image compression"
      ],
      "metadata": {
        "id": "XqwuEq0Si7_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overlapping clustering\n",
        "--> in this data point belongs to multiple clusters with separate degrees of membership\n",
        "example:\n",
        "1 soft or fuzzy k-means clustering"
      ],
      "metadata": {
        "id": "XtfVEk0OjYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hierarchical clustering\n",
        "--> aka hierarchical cluster analysis (HCA)\n",
        "--> Agglomerative or Divisive\n",
        "\n",
        "\n",
        "--> Agglomerative clustering\n",
        "\n",
        "1. bottom up approach\n",
        "2. initially datapoints isolated as seperate groupings\n",
        "3. they are merged together iteratively on the basis of similarity until one cluster has been achieved.\n",
        "4. four different methods are commonly used to measure similarity:\n",
        "   \n",
        "   1. wards linkage\n",
        "   his method states that the distance between two clusters is defined by\n",
        "   the increase in the sum of squared after the clusters are merged.\n",
        "   \n",
        "   2. average linkage\n",
        "   this method is defined by the average distance between two points in each cluster\n",
        "   \n",
        "   3. complete (or maximum) linkage\n",
        "   this method is defined by the maximum distance between two points in each cluster\n",
        "   \n",
        "   4. single (or minimum) linkage\n",
        "   this method is defined by the minimum distance betweent two points in each cluster\n",
        "\n",
        "NOTE: Euclidean distance is the most common metric used to calculate these distances\n",
        "      Manhatten distance is also used for finding metrics\n",
        "\n",
        "--> Divisive\n",
        "1. top down approach\n",
        "2. all datapoints are considered as a single cluster, this is divided based on the differences between data poings.\n",
        "3. Not commonly used.\n",
        "\n",
        "Note: these clustering processes are visualized by using a dendrogram, a tree like diagram that documents \n",
        "      the merging or splittin of data points at each iteration"
      ],
      "metadata": {
        "id": "Mq_APRn3jtIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilistic clustering\n",
        "\n",
        "--> helps to solve density estimation or soft clustering problems\n",
        "--> In this, data points are clustered based on the likelihood that they belong to a particular\n",
        "distribution.\n",
        "\n",
        "example: Gaussian Mixture Models\n",
        "\n",
        "1. these are mixture models, means they are made up of an unspecified number of probability distributions\n",
        "2. GMM's 'are primarily leveraged to determine which gaussian, or normal, probability distribution a given data point belongs to.\n",
        "3. if the mean and sd is known we can know the distribution, but in GMM we can't know this,  so we assume that a latent, or hidden, \n",
        "   variable exists to cluste data points appropriately.\n",
        "4. it is not required to use the Expectation-Maximization (EM) algorithm, it is a commonly used to estimate\n",
        "   the assignment probabilities for a given data point to a particular data cluster.\n"
      ],
      "metadata": {
        "id": "1R3iR6Pyry21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Association Rules\n",
        "\n",
        "--> finding relations between variables in a given dataset.\n",
        "--> frequently used for masket basket analysis, allowng to find the relationship between the different products.\n",
        "--> enables businesses to the cross selling and recommendation system.\n",
        "--> ex: amazon's 'customers  who bought also bought'' and spotify's' Discover weekly playlist\n",
        "\n",
        "Examples:\n",
        "1. Apriori algorithm  (uses hash tree to count itemsets, navigating through the dataset in a breadth-first-manner)\n",
        "2. Eclat\n",
        "3. FP-Growth\n"
      ],
      "metadata": {
        "id": "lEzg1lGWuhqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dimensionality Reduction\n",
        "--> more data yields more accurate results.but it can also impact the performance of the ml model(overfittin)\n",
        "--> it's hard to visualize datasets of huge size.\n",
        "--> reduces the data inputs to a manageable size while also preserving the dataset as much as possible.\n",
        "--> It is commonly used in preprocessing stage.\n",
        "--> few dimensionality reduction techniques are\n",
        "      1. Principle Component Analysis (PCA)\n",
        "      2. Singular Value Decompostion (SVD)\n",
        "      3. AutoEncoders\n",
        "\n",
        "Principle Component Analysis(PCA)\n",
        "--> reduces the redundancies and to compress datasets throught feature extraction.\n",
        "--> uses linear transformation to create a new data representation\n",
        "--> yields a set of 'principle components'.\n",
        "--> first principle component is the direction which maximizes the variance of the dataset.\n",
        "--> while the second principle component also finds the max variance in the data, it is completely uncorelated to the first principal component, yielding a direction perpendicular or orthogonal, to the first component.\n",
        "--> This process repeats based on the number of dimensions, where a next principle component is the direction orthogonal to the prior components with the most variance.\n",
        "\n",
        "Singular Value Decompostion(SVD)\n",
        "--> Another dimensionality reduction approach which factorizes a matrix, A into three low rank matrices.\n",
        "--> SVD is denoted by the formula, A=USVT, where u and v are othogonal matrices. S is a diagonal matrix and S values are considered singular values of matrix A.\n",
        "--> commonly used to reduce noise and compress data, such as image files.\n",
        "\n",
        "AutoEncoders:\n",
        "--> leverage neural networks to compress data and then recreate a new representation of the original data's' input\n",
        "--> the hidden layer act as a bottleneck to compress the input layer prior to reconstructing within the output layer\n",
        "--> The stage rom the input layer to the hidden layer is called \"encoding\" and the hidden layer to the ouput layer is called as \"decoding\""
      ],
      "metadata": {
        "id": "QdJ6Z2C49c9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Applications of unsupervised learning\n",
        "1. News Sections\n",
        "2. Customer segmentation\n",
        "3. Anamoly detection\n",
        "4. Recommendation Engines\n",
        "5. Medical imaging\n",
        "6. computer vision"
      ],
      "metadata": {
        "id": "lJxYkdjyB6QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervised vs unsupervised\n",
        "1. supervised is on labeled data, it requires to label the data, \n",
        "2. its accuracy is more compared to the unsupervised\n",
        "3. they require upfront human intervention to label the data appropriately.\n",
        "4. this labeled dataset reduces the complexity of model training for supervised algos\n",
        "\n",
        "Then why we prefer unsupervised over supervised\n",
        "--> semi-supervised learing occurs wen only part of the given input data has been labeled.\n",
        "--> unsupervised and semi-supervised are more appealing alternatives as it can be time-consuming and costly to rely on domain expertise\n",
        "    to label data appropriately for supervised learning."
      ],
      "metadata": {
        "id": "ksi4SmQrFHcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Complexities of unsupervised\n",
        "1. requires more amount of data\n",
        "2. consumes more time to train.\n",
        "3. no scope for accuracy evaluation.\n",
        "4. needs human intervention for validation.\n",
        "5. lack of transparency basis on which data is clustered."
      ],
      "metadata": {
        "id": "VKcXvBYGHcMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}