{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP82v71qefGv/aTNRB0/b/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilabhimanyu/Data-Science/blob/main/ML_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Algorithms\n",
        "\n",
        "1. Ordinary Least Squares Regression (OLSR)\n",
        "2. Linear Regression\n",
        "3. Logistic Regression\n",
        "4. Stepwise Regression\n",
        "5. Multivariate Adaptive Regression Splines (MARS)\n",
        "6. Locally Estimated Scatterplot Smoothing (LOESS)\n"
      ],
      "metadata": {
        "id": "HLZ-6gMXFv2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instance Based Algorithms\n",
        "\n",
        "1. K-Neares Neighbors (KNN)\n",
        "2. Learning Vector Quantization (LVQ)\n",
        "3. Self-Organizing Map (SOM)\n",
        "4. Locally Weighted Learning (LWL)\n",
        "5. Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "LQyYm1HdKD_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization Algorithms\n",
        "\n",
        "1. Ridge Regression\n",
        "2. Least Absolute Shrinkage and Selection Operator (LASSO)\n",
        "3. Elastic Net\n",
        "4. Least-Angle Regression (LARS)\n"
      ],
      "metadata": {
        "id": "_yO-oejsNnRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Algorithms\n",
        "\n",
        "1. Classification and Regression Tree (CART)\n",
        "2. Iterative Dichotomiser 3 (ID3)\n",
        "3. C4.5 and C5.0 (different versions of a powerful approach)\n",
        "4. Chi-squared Automatic Interaction Detection (CHAID)\n",
        "5. Decision Stump\n",
        "6. M5\n",
        "7. Conditional Decision Trees\n"
      ],
      "metadata": {
        "id": "A03yZKz_Np7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Algorithms\n",
        "The most popular Bayesian algorithms are:\n",
        "\n",
        "1. Naive Bayes\n",
        "2. Gaussian Naive Bayes\n",
        "3. Multinomial Naive Bayes\n",
        "4. Averaged One-Dependence Estimators (AODE)\n",
        "5. B5 and C5.0 (ayesian Belief Network (BBN)\n",
        "6. Bayesian Network (BN)"
      ],
      "metadata": {
        "id": "Z0Qt0KoWNuVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Association Rules Algorithms\n",
        "\n",
        "The most popular association rule learning algorithms are:\n",
        "\n",
        "1. Apriori algorithm\n",
        "2. Eclat algorithm"
      ],
      "metadata": {
        "id": "KFU9jEAkNxS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artificial Neural Network algorithms\n",
        "\n",
        "The most popular artificial neural network algorithms are:\n",
        "\n",
        "1. Perceptron\n",
        "2. Multilayer Perceptrons (MLP)\n",
        "3. Back-Propagation\n",
        "4. Stochastic Gradient Descent\n",
        "5. Hopfield Network\n",
        "6. Radial Basis Function Network (RBFN)\n"
      ],
      "metadata": {
        "id": "EeWY_MHcN1vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "This can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.\n",
        "\n",
        "1. Principal Component Analysis (PCA)\n",
        "2. Principal Component Regression (PCR)\n",
        "3. Partial Least Squares Regression (PLSR)\n",
        "4. Sammon Mapping\n",
        "5. Multidimensional Scaling (MDS)\n",
        "6. Projection Pursuit\n",
        "7. Linear Discriminant Analysis (LDA)\n",
        "8. Mixture Discriminant Analysis (MDA)\n",
        "9. Quadratic Discriminant Analysis (QDA)\n",
        "10.Flexible Discriminant Analysis (FDA)"
      ],
      "metadata": {
        "id": "61TQ-doXN6Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning Algorithms\n",
        "The most popular deep learning algorithms are:\n",
        "\n",
        "1. Convolutional Neural Network (CNN)\n",
        "2. Recurrent Neural Networks (RNNs)\n",
        "3. Long Short-Term Memory Networks (LSTMs)\n",
        "4. Stacked Auto-Encoders\n",
        "5. Deep Boltzmann Machine (DBM)\n",
        "6. Deep Belief Networks (DBN)\n",
        "7. Generative Adaptive Neural Networks (GAN networks)\n",
        "8. Transformers"
      ],
      "metadata": {
        "id": "c1mRUBLuN9Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble Learning Algoritms\n",
        "Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.\n",
        "\n",
        "1. Boosting\n",
        "2. Bootstrapped Aggregation (Bagging)\n",
        "3. AdaBoost\n",
        "4  CATBoost\n",
        "5. Weighted Average (Blending)\n",
        "6. Stacked Generalization (Stacking)\n",
        "7. Gradient Boosting Machines (GBM)\n",
        "8. Gradient Boosted Regression Trees (GBRT)\n",
        "9. Random Forest\n"
      ],
      "metadata": {
        "id": "ztCtTX7BKmMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}